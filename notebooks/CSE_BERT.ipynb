{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSE_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jbUoKrggtU2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkh3gW60e9L3"
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwGnlZt9fA3n"
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4VHXB-mf9OP"
      },
      "source": [
        "DATA_COLUMN = 'Text'\n",
        "LABEL_COLUMN = 'HandLabels'\n",
        "label_list = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "path = '/content/drive/My Drive/clean/'\n",
        "languages = ['Hungarian', 'Portuguese', 'Bosnian',\n",
        "             'Croatian', 'Polish', 'Russian',\n",
        "             'Serbian', 'Slovak', 'Slovenian',\n",
        "             'English', 'German', 'Swedish']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KZzS6k4f9UW"
      },
      "source": [
        "def to_number(text_labels):\n",
        "    labels = []\n",
        "    for l in text_labels:\n",
        "        n = 0\n",
        "        if l == 'Neutral': n = 1\n",
        "        elif l == 'Negative': n = 2\n",
        "        labels.append(n)\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-27j-Ftpf9WQ"
      },
      "source": [
        "def load_dataset(train_langs, test_langs, train_on_test_lang=False):\n",
        "    \n",
        "    # Load train languages\n",
        "    df_train = pd.DataFrame()\n",
        "    for lang in train_langs:\n",
        "        df = pd.read_csv(path + lang + '.csv')\n",
        "        df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "        df.dropna(axis=0, inplace=True)\n",
        "        df_train = pd.concat([df_train, df], ignore_index=True)\n",
        "        print(df.shape)\n",
        "    \n",
        "    df_train.reset_index(drop=True, inplace=True)\n",
        "    print(df_train.shape)\n",
        "    \n",
        "    # Load test language\n",
        "    df_test = pd.read_csv(path + test_lang + '.csv')\n",
        "    df_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    df_test.dropna(axis=0, inplace=True)\n",
        "    df_test.reset_index(drop=True, inplace=True)\n",
        "    print(df_test.shape)\n",
        "    \n",
        "    if train_on_test_lang:\n",
        "        train_size = 0.3\n",
        "        num_train = int(df_test.shape[0] * train_size)\n",
        "        df_train = pd.concat([df_train, df_test.head(num_train)], ignore_index=True)\n",
        "        df_test = df_test.tail(df_test.shape[0]-num_train)\n",
        "    \n",
        "    df_train['HandLabels'] = to_number(df_train['HandLabels'])\n",
        "    df_test['HandLabels'] = to_number(df_test['HandLabels'])\n",
        "\n",
        "    #train_texts = list(df_train['Text'])\n",
        "    #train_labels = to_number(df_train['HandLabels'])\n",
        "\n",
        "    #test_texts = list(df_test['Text'])\n",
        "    #test_labels = to_number(df_test['HandLabels'])\n",
        "\n",
        "    df_train.columns = ['text', 'labels']\n",
        "    df_test.columns = ['text', 'labels']\n",
        "\n",
        "    print(df_train.head())\n",
        "    print(df_test.head())\n",
        "    \n",
        "    return df_train, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gg0KaVQf9YN"
      },
      "source": [
        "# define evaluation metrics average F1 score\n",
        "def avg_f1_score(y_true, y_pred):\n",
        "    scores = f1_score(y_true, y_pred, average=None)\n",
        "    print(y_true[:10])\n",
        "    print(y_pred[:10])\n",
        "    print(scores)\n",
        "    # get average F1 for postive and negative F1 scores\n",
        "    f1_negative = scores[2] # Negative\n",
        "    f1_positive = scores[0] # Positive\n",
        "    return (f1_negative + f1_positive) / 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZD75tzjgO5J"
      },
      "source": [
        "def eval(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    avg_f1 = avg_f1_score(y_true, y_pred)\n",
        "    return \"acc:{} f1:{}\".format(acc, avg_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boXLfjeIgggk"
      },
      "source": [
        "def fit_and_evaluate(train_langs, test_lang):\n",
        "    print(train_langs, test_lang)\n",
        "    #model_name = 'bert-base-multilingual-cased'\n",
        "    model_name = 'EMBEDDIA/crosloengual-bert'\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    transformers_logger = logging.getLogger('transformers')\n",
        "    transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "    # Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns.\n",
        "    # If the Dataframe has a header, it should contain a 'text' and a 'labels' column.\n",
        "    # If no header is present, the Dataframe should contain at least two columns,\n",
        "    # with the first column is the text with type str, and the second column in the label with type int.\n",
        "    df_train, df_test = load_dataset(train_langs, test_lang, train_on_test_lang=True)\n",
        "\n",
        "\n",
        "    # hyperparameters\n",
        "    model_args = ClassificationArgs()\n",
        "    model_args.num_train_epochs = 1\n",
        "    model_args.learning_rate = 1e-4\n",
        "    model_args.train_batch_size = 32\n",
        "    model_args.overwrite_output_dir = True\n",
        "    model_args.train_custom_parameters_only = True\n",
        "    model_args.custom_parameter_groups = [\n",
        "        {\n",
        "            \"params\": [\"classifier.weight\"],\n",
        "            \"lr\": 1e-4,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\"classifier.bias\"],\n",
        "            \"lr\": 1e-4,\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Create a ClassificationModel\n",
        "    model = ClassificationModel('bert', model_name, num_labels=3, args=model_args)\n",
        "    print(model.get_named_parameters())\n",
        "\n",
        "    # Train the model\n",
        "    print('Training ...')\n",
        "    model.train_model(df_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print('Evaluating ...')\n",
        "    predictions, raw_outputs = model.predict(df_test['text'].values)\n",
        "    out = eval(df_test['labels'].values, predictions)\n",
        "\n",
        "    # write results to file\n",
        "    with open('/content/drive/My Drive/results/results.txt', 'a+') as f:\n",
        "        f.write(\"{} {} {}\\n\".format(train_langs, test_lang, out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuamOwWgggnj"
      },
      "source": [
        "experiments_same_fam = [\n",
        "    (['English'], 'German'),\n",
        "    (['Serbian'], 'Slovenian'),\n",
        "    (['Serbian'], 'Croatian'),\n",
        "    (['Serbian'], 'Bosnian'),\n",
        "    (['Polish'], 'Slovenian'),\n",
        "    (['Slovak'], 'Slovenian'),\n",
        "    (['Croatian'], 'Slovenian'),\n",
        "    (['Croatian'], 'Serbian'),\n",
        "    (['Croatian'], 'Bosnian'),\n",
        "    (['Slovenian'], 'Croatian'),\n",
        "    (['Slovenian'], 'Serbian'),\n",
        "    (['Slovenian'], 'Bosnian'),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi_qbKTDgjNj"
      },
      "source": [
        "experiments_diff_lang_fam = [\n",
        "    (['German'], 'Slovenian'),\n",
        "    (['English'], 'Slovenian'),\n",
        "    (['Swedish'], 'Slovenian'),\n",
        "    (['Hungarian'], 'Slovenian'),\n",
        "    (['Portuguese'], 'Slovenian'),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37n39FyDgj7m"
      },
      "source": [
        "experiments_large_train_dataset = [\n",
        "    (['Croatian', 'Serbian', 'Bosnian'], 'Slovenian'),\n",
        "    (['English', 'Swedish'], 'German'),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRcugnimgkAP"
      },
      "source": [
        "for (train_langs, test_lang) in experiments_same_fam:\n",
        "    fit_and_evaluate(train_langs, test_lang)\n",
        "\n",
        "for (train_langs, test_lang) in experiments_diff_lang_fam:\n",
        "    fit_and_evaluate(train_langs, test_lang)\n",
        "\n",
        "for (train_langs, test_lang) in experiments_large_train_dataset:\n",
        "    fit_and_evaluate(train_langs, test_lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezQJQJvjaKW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}