{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "source": [
        "# Multilingual Twitter sentiment analysis with BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TExQ5Lu53YhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "2c9612c3-30b7-464e-b7d1-16b4c9e7f32f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "c171e517-d6e1-4613-e653-c9e332e52442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "a827217f-8612-417e-b13b-409f02d6eeec"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import io\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "#import tf_metrics\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 07:09:46.743827 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "434e41d6-49f8-4db9-ea76-79e0b11f9dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "\n",
        "OUTPUT_DIR = 'output'#@param {type:\"string\"}\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('Model output directory: {}'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model output directory: output\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJeJPVq6VtHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/clean_data/'\n",
        "languages = ['Hungarian', 'Portuguese', 'Bosnian', 'Croatian', 'Polish', 'Russian', 'Serbian', 'Slovak', 'Slovenian', 'English', 'German', 'Swedish']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5nvMyT6lCVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_langauge_pretraining(train_lang, test_lang):\n",
        "    \n",
        "    # Load train languages\n",
        "    df_train = pd.DataFrame()\n",
        "    for lang in train_lang:\n",
        "        df = pd.read_csv(path + train_lang + '.csv')\n",
        "        df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "        df.dropna(axis=0, inplace=True)\n",
        "        df_train = pd.concat([df_train, df], ignore_index=True)\n",
        "        print(df.shape)\n",
        "    \n",
        "    df_train.reset_index(drop=True, inplace=True)\n",
        "    print(df_train.shape)\n",
        "        \n",
        "    \n",
        "    # Load test language\n",
        "    df_train = pd.read_csv(path + train_lang + '.csv')\n",
        "    df_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    df_train.dropna(axis=0, inplace=True)\n",
        "    df_train.reset_index(drop=True, inplace=True)\n",
        "    print(df_test.shape)\n",
        "    \n",
        "    train = df_train\n",
        "    test = df_test\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-0tw45TwYAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_language(lang):\n",
        "    \n",
        "    df = pd.read_csv(path + lang + '.csv')\n",
        "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    df.dropna(axis=0, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    train_ix, test_ix = train_test_split(df.index, test_size=0.3)\n",
        "    train = df.iloc[train_ix]\n",
        "    test = df.iloc[test_ix]\n",
        "    return train, test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'Text'\n",
        "LABEL_COLUMN = 'HandLabels'\n",
        "label_list = ['Positive', 'Negative', 'Neutral']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "128a9db0-928f-4760-8973-496789af195c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    with tf.Graph().as_default():\n",
        "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "        with tf.Session() as sess:\n",
        "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])\n",
        "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0731 07:09:59.389066 140125504415616 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0731 07:10:02.536997 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "1. Load BERT\n",
        "2. Create a single new layer for finetunning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bmYDLa5OYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define evaluation metrics average F1 score\n",
        "def avg_f1_score(y_true, y_pred, encoder_dict):\n",
        "    scores = f1_score(y_true, y_pred, average=None)\n",
        "    # get average F1 for postive and negative F1 scores\n",
        "    f1_negative = scores[0] # Negative\n",
        "    f1_positive = scores[2] # Positive\n",
        "    return (f1_negative + f1_positive) / 2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels):\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
        "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Use \"sequence_outputs\" for token-level output.\n",
        "    output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    # Create our own layer to tune for politeness data.\n",
        "    output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "    \n",
        "    with tf.variable_scope(\"loss\"):\n",
        "\n",
        "        # Dropout helps prevent overfitting\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "        # Convert labels into one-hot encoding\n",
        "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "        if is_predicting:\n",
        "            return (predicted_labels, log_probs)\n",
        "\n",
        "        # If we're train/eval, compute loss between predicted and actual label\n",
        "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "        return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "\n",
        "            (loss, predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "            train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "            # Calculate evaluation metrics. \n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "\n",
        "                return {\n",
        "                    \"eval_accuracy\": accuracy,\n",
        "                }\n",
        "\n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            predictions = {\n",
        "                'probabilities': log_probs,\n",
        "                'labels': predicted_labels\n",
        "            }\n",
        "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 2.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dOeypFcf3LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_and_evaluate(train, test):\n",
        "    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "    train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                       text_a = x[DATA_COLUMN], \n",
        "                                                                       text_b = None, \n",
        "                                                                       label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "    test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                       text_a = x[DATA_COLUMN], \n",
        "                                                                       text_b = None, \n",
        "                                                                       label = x[LABEL_COLUMN]), axis = 1)\n",
        "    \n",
        "    # We'll set sequences to be at most 128 tokens long.\n",
        "    MAX_SEQ_LENGTH = 128\n",
        "    # Convert our train and test features to InputFeatures that BERT understands.\n",
        "    train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    \n",
        "    \n",
        "    # Compute # train and warmup steps from batch size\n",
        "    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "    \n",
        "    # Specify output directory and number of checkpoint steps to save\n",
        "    run_config = tf.estimator.RunConfig(\n",
        "        model_dir=OUTPUT_DIR,\n",
        "        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "    \n",
        "    \n",
        "    model_fn = model_fn_builder(\n",
        "        num_labels=len(label_list),\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        num_train_steps=num_train_steps,\n",
        "        num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "    # Create an input function for training.\n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "    \n",
        "    print(f'Beginning Training!')\n",
        "    current_time = datetime.now()\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "    print(\"Training took time \", datetime.now() - current_time)\n",
        "    \n",
        "    \n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "        features=test_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNT51433ZWjv",
        "colab_type": "code",
        "outputId": "82acd9af-02c5-4aed-ffe7-95075d8a9710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# EXPERIMENT 1\n",
        "\n",
        "languages = ['Bosnian', 'Bolgarian']\n",
        "# for each language evaluate BERT on only 1 language dataset\n",
        "for lang in languages:\n",
        "    print(lang + '----------------------------------------------')\n",
        "    train, test = load_language(lang)\n",
        "    fit_and_evaluate(train, test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bosnian----------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 07:10:04.288084 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0731 07:10:04.289157 140125504415616 run_classifier.py:774] Writing example 0 of 26673\n",
            "I0731 07:10:04.290577 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:04.291525 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:04.292448 140125504415616 run_classifier.py:464] tokens: [CLS] Su ##kob demo ##nst ##rana ##ta i poli ##cije u Ki ##jev ##u [SEP]\n",
            "I0731 07:10:04.293177 140125504415616 run_classifier.py:465] input_ids: 101 12271 62545 30776 40300 20906 10213 177 91929 13303 189 28941 24873 10138 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.293923 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.295881 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.296980 140125504415616 run_classifier.py:468] label: Negative (id = 1)\n",
            "I0731 07:10:04.299236 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:04.300104 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:04.302126 140125504415616 run_classifier.py:464] tokens: [CLS] H ##A ##HA ##HA ##H ##HA ##A ##H ##HA ##HA ##HA ##HA ##H ##HA ##HA ##HA ##HA ##HA ##H ##HA A NE MO ##G ##U [SEP]\n",
            "I0731 07:10:04.303758 140125504415616 run_classifier.py:465] input_ids: 101 145 10738 58132 58132 12396 58132 10738 12396 58132 58132 58132 58132 12396 58132 58132 58132 58132 58132 12396 58132 138 86607 102059 11447 12022 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.305867 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.306962 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.309536 140125504415616 run_classifier.py:468] label: Neutral (id = 2)\n",
            "I0731 07:10:04.311371 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:04.312162 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:04.314079 140125504415616 run_classifier.py:464] tokens: [CLS] Ko je Ivan Hr ##ka ##ć [UNK] Č ##iko ##ta : Za ##ro ##blje ##nike je tjera ##o na sp ##ol ##ne od ##nose i gas ##io im ci ##gare ##te na jeziku Iako je u . . . [SEP]\n",
            "I0731 07:10:04.315738 140125504415616 run_classifier.py:465] input_ids: 101 30186 10144 15631 74968 10371 11484 100 294 18924 10213 131 14074 10567 101311 37385 10144 45422 10133 10132 32650 11481 10238 10311 53638 177 16091 10638 10211 11322 30671 10216 10132 42758 37147 10144 189 119 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.316767 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.318609 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.320077 140125504415616 run_classifier.py:468] label: Negative (id = 1)\n",
            "I0731 07:10:04.322387 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:04.323465 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:04.324544 140125504415616 run_classifier.py:464] tokens: [CLS] : Po ##če ##o vik ##end , od ##oh os ##nova ##t ' nek ##u N ##VO . [SEP]\n",
            "I0731 07:10:04.326253 140125504415616 run_classifier.py:465] input_ids: 101 131 11255 15015 10133 56324 19342 117 10311 25686 10427 23382 10123 112 53334 10138 151 70325 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.335980 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.337154 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.338088 140125504415616 run_classifier.py:468] label: Neutral (id = 2)\n",
            "I0731 07:10:04.340220 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:04.341456 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:04.342478 140125504415616 run_classifier.py:464] tokens: [CLS] ka ##zu da kad si od ##mora ##n i nas ##pa ##van da si naj ##lje ##psi , o ##ci ##gled ##no nisu vid ##jeli tv ##iter ##ase [SEP]\n",
            "I0731 07:10:04.343482 140125504415616 run_classifier.py:465] input_ids: 101 10730 13078 10143 14753 10294 10311 79374 10115 177 12947 11359 12955 10143 10294 39158 21235 73421 117 183 10598 38239 10343 24184 11618 34504 19767 27295 16896 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.344984 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.346573 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:04.348197 140125504415616 run_classifier.py:468] label: Positive (id = 0)\n",
            "I0731 07:10:07.241725 140125504415616 run_classifier.py:774] Writing example 10000 of 26673\n",
            "I0731 07:10:10.106200 140125504415616 run_classifier.py:774] Writing example 20000 of 26673\n",
            "I0731 07:10:12.328817 140125504415616 run_classifier.py:774] Writing example 0 of 11432\n",
            "I0731 07:10:12.330146 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:12.330941 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:12.334469 140125504415616 run_classifier.py:464] tokens: [CLS] Sad sam se iz ##ner ##vira ##la više ne ##ću da g ##leda ##m dne ##vnik poc ##rka ##li dab ##og ##da . [SEP]\n",
            "I0731 07:10:12.335906 140125504415616 run_classifier.py:465] input_ids: 101 48691 21083 10126 10793 11129 90414 10330 16123 10554 23873 10143 175 89153 10147 29747 48786 32398 20266 10390 97141 12717 10229 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.339326 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.340219 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.341947 140125504415616 run_classifier.py:468] label: Negative (id = 1)\n",
            "I0731 07:10:12.343179 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:12.344232 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:12.345172 140125504415616 run_classifier.py:464] tokens: [CLS] Toma ##ši ##ca - rezultat ' raz ##d ##vaja ##nja naroda ' [SEP]\n",
            "I0731 07:10:12.346184 140125504415616 run_classifier.py:465] input_ids: 101 78079 14912 10425 118 48306 112 20710 10162 56234 12582 50677 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.347093 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.348449 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.349437 140125504415616 run_classifier.py:468] label: Neutral (id = 2)\n",
            "I0731 07:10:12.350453 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:12.351367 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:12.352254 140125504415616 run_classifier.py:464] tokens: [CLS] 10 u 10 - 11 . 12 . 2013 . [SEP]\n",
            "I0731 07:10:12.353107 140125504415616 run_classifier.py:465] input_ids: 101 10150 189 10150 118 10193 119 10186 119 10207 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.354022 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.355150 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.356057 140125504415616 run_classifier.py:468] label: Neutral (id = 2)\n",
            "I0731 07:10:12.357409 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:12.358251 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:12.359004 140125504415616 run_classifier.py:464] tokens: [CLS] Vol ##im svoje kr ##eat ##ivno pl ##odne periode . Jedi ##no tad sve kao i da ima nekog sm ##is ##la . [SEP]\n",
            "I0731 07:10:12.359914 140125504415616 run_classifier.py:465] input_ids: 101 12546 11759 16496 50302 52064 93393 20648 69155 17715 119 61340 10343 50586 17202 11177 177 10143 13872 58406 39709 10291 10330 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.360836 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.361765 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.365737 140125504415616 run_classifier.py:468] label: Positive (id = 0)\n",
            "I0731 07:10:12.366701 140125504415616 run_classifier.py:461] *** Example ***\n",
            "I0731 07:10:12.367693 140125504415616 run_classifier.py:462] guid: None\n",
            "I0731 07:10:12.368712 140125504415616 run_classifier.py:464] tokens: [CLS] Photos ##et : [SEP]\n",
            "I0731 07:10:12.369732 140125504415616 run_classifier.py:465] input_ids: 101 66090 10308 131 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.370786 140125504415616 run_classifier.py:466] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.372206 140125504415616 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0731 07:10:12.373238 140125504415616 run_classifier.py:468] label: Neutral (id = 2)\n",
            "I0731 07:10:15.254909 140125504415616 run_classifier.py:774] Writing example 10000 of 11432\n",
            "I0731 07:10:15.667714 140125504415616 estimator.py:209] Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f712ba573c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0731 07:10:15.694997 140125504415616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0731 07:10:28.988588 140125504415616 estimator.py:1145] Calling model_fn.\n",
            "I0731 07:10:31.870772 140125504415616 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0731 07:10:32.004801 140125504415616 deprecation.py:506] From <ipython-input-13-0b1934a6b7e1>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0731 07:10:32.041664 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0731 07:10:32.043170 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0731 07:10:32.051713 140125504415616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0731 07:10:32.065450 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0731 07:10:32.297467 140125504415616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0731 07:10:35.843395 140125504415616 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0731 07:10:40.216978 140125504415616 estimator.py:1147] Done calling model_fn.\n",
            "I0731 07:10:40.219564 140125504415616 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0731 07:10:43.901736 140125504415616 monitored_session.py:240] Graph was finalized.\n",
            "I0731 07:10:51.277251 140125504415616 session_manager.py:500] Running local_init_op.\n",
            "I0731 07:10:51.462214 140125504415616 session_manager.py:502] Done running local_init_op.\n",
            "I0731 07:11:07.436493 140125504415616 basic_session_run_hooks.py:606] Saving checkpoints for 0 into output/model.ckpt.\n",
            "I0731 07:11:24.752498 140125504415616 basic_session_run_hooks.py:262] loss = 1.1007011, step = 0\n",
            "I0731 07:13:06.443548 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 0.983364\n",
            "I0731 07:13:06.445100 140125504415616 basic_session_run_hooks.py:260] loss = 1.1420748, step = 100 (101.693 sec)\n",
            "I0731 07:14:34.688097 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13321\n",
            "I0731 07:14:34.689480 140125504415616 basic_session_run_hooks.py:260] loss = 0.8635232, step = 200 (88.244 sec)\n",
            "I0731 07:16:02.944287 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13307\n",
            "I0731 07:16:02.945782 140125504415616 basic_session_run_hooks.py:260] loss = 0.85075736, step = 300 (88.256 sec)\n",
            "I0731 07:17:31.203036 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13303\n",
            "I0731 07:17:31.204600 140125504415616 basic_session_run_hooks.py:260] loss = 0.9063981, step = 400 (88.259 sec)\n",
            "I0731 07:18:58.534631 140125504415616 basic_session_run_hooks.py:606] Saving checkpoints for 500 into output/model.ckpt.\n",
            "I0731 07:19:09.858733 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.01363\n",
            "I0731 07:19:09.860026 140125504415616 basic_session_run_hooks.py:260] loss = 0.86449295, step = 500 (98.655 sec)\n",
            "I0731 07:20:38.271985 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13105\n",
            "I0731 07:20:38.273416 140125504415616 basic_session_run_hooks.py:260] loss = 0.80163944, step = 600 (88.413 sec)\n",
            "I0731 07:22:06.518390 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13319\n",
            "I0731 07:22:06.520122 140125504415616 basic_session_run_hooks.py:260] loss = 0.6131195, step = 700 (88.247 sec)\n",
            "I0731 07:23:34.804377 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13268\n",
            "I0731 07:23:34.805905 140125504415616 basic_session_run_hooks.py:260] loss = 0.8275726, step = 800 (88.286 sec)\n",
            "I0731 07:25:03.131083 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13216\n",
            "I0731 07:25:03.132597 140125504415616 basic_session_run_hooks.py:260] loss = 0.82467806, step = 900 (88.327 sec)\n",
            "I0731 07:26:30.603228 140125504415616 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into output/model.ckpt.\n",
            "I0731 07:26:41.050360 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.02125\n",
            "I0731 07:26:41.051814 140125504415616 basic_session_run_hooks.py:260] loss = 0.53510344, step = 1000 (97.919 sec)\n",
            "I0731 07:28:09.452003 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.1312\n",
            "I0731 07:28:09.453482 140125504415616 basic_session_run_hooks.py:260] loss = 0.48325044, step = 1100 (88.402 sec)\n",
            "I0731 07:29:37.768520 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13229\n",
            "I0731 07:29:37.770045 140125504415616 basic_session_run_hooks.py:260] loss = 0.48789272, step = 1200 (88.317 sec)\n",
            "I0731 07:31:06.205902 140125504415616 basic_session_run_hooks.py:692] global_step/sec: 1.13074\n",
            "I0731 07:31:06.207409 140125504415616 basic_session_run_hooks.py:260] loss = 0.29906672, step = 1300 (88.437 sec)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}